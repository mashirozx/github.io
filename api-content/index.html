{"posts":[{"title":"AWS CLI Notes","content":"General configuration aws configure AWS Access Key ID [None]: Q3AM3UQ867SPQQA43P2F AWS Secret Access Key [None]: zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG Default region name [None]: us-east-1 Default output format [None]: ENTER S3 # enable AWS Signature Version'4' (minio need) aws configure set default.s3.signature_version s3v4 # paramater if not use a aws S3 stroage aws --endpoint-url https://s3.host s3 [command] # list buckets aws s3 ls # list files in bucket aws s3 ls &lt;S3Uri&gt; # create bucket aws s3 mb &lt;S3Uri&gt; # delete bucket aws s3 rb &lt;S3Uri&gt; # copy (upload/download) file aws s3 cp &lt;LocalPath&gt; &lt;S3Uri&gt; or &lt;S3Uri&gt; &lt;LocalPath&gt; or &lt;S3Uri&gt; &lt;S3Uri&gt; aws s3 cp example.tar.gz s3://mybucket # When passed with the parameter --recursive, the following cp command # recursively copies all files under a specified directory to a specified # bucket and prefix while excluding some files by using an --exclude # parameter. In this example, the directory myDir has the files test1.txt # and test2.jpg: aws s3 cp myDir s3://mybucket/ --recursive --exclude &quot;*.jpg&quot; aws s3 cp s3://mybucket/ s3://mybucket2/ --recursive --exclude &quot;another/*&quot; # You can combine --exclude and --include options to copy only objects # that match a pattern, excluding all others aws s3 cp s3://mybucket/logs/ s3://mybucket2/logs/ --recursive --exclude &quot;*&quot; --include &quot;*.log&quot; # Setting the Access Control List (ACL) while copying an S3 object # paramaters (https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl): # private, public-read, public-read-write, authenticated-read, aws-exec-read, # bucket-owner-read, bucket-owner-full-control and log-delivery-write aws s3 cp s3://mybucket/test.txt s3://mybucket/test2.txt --acl public-read-write # move file aws s3 mv &lt;LocalPath&gt; &lt;S3Uri&gt; or &lt;S3Uri&gt; &lt;LocalPath&gt; or &lt;S3Uri&gt; &lt;S3Uri&gt; # delete file aws s3 rm &lt;S3Uri&gt; aws s3 rm s3://mybucket/example.tar.gz # sync directory # Syncs directories and S3 prefixes. Recursively copies new and updated # files from the source directory to the destination. Only creates folders # in the destination if they contain one or more files. aws s3 sync &lt;LocalPath&gt; &lt;S3Uri&gt; or &lt;S3Uri&gt; &lt;LocalPath&gt; or &lt;S3Uri&gt; &lt;S3Uri&gt; [--delete] # --delete (boolean): Files that exist in the destination but not in the # source are deleted during sync. Reference: https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3 ","link":"https://mashiro.top/post/aws-cli-notes/"},{"title":"Mastodon Notes","content":"Commands docker exec -it mastodon_web_1 bash # clean cached media files tootctl media remove # emoji CLI tootctl emoji import ./examplr.tar.gz [--prefix PREFIX] [--suffix SUFFIX] [--overwrite] [--unlisted] [--category CATEGORY] External Resources Using the admin CLI: https://docs.joinmastodon.org/admin/tootctl/ The official Docker guideÔºöhttps://github.com/tootsuite/documentation/blob/archive/Running-Mastodon/Docker-Guide.md Nginx part (Chinese): https://candinya.com/posts/mastodon-first-meet/ Docker configurations The project now includes a Dockerfile and a docker-compose.yml file (which requires at least docker-compose version 1.10.0). Prerequisites Working basic (Linux) server with Nginx (or Apache2; not officially supported). Recent stable version of Docker. Recent stable version of Docker-compose. Setting up Clone Mastodon's repository. # Clone mastodon to ~/live directory git clone https://github.com/tootsuite/mastodon.git live # Change directory to ~/live cd ~/live # Checkout to the latest stable branch git checkout $(git tag -l | grep -v 'rc[0-9]*$' | sort -V | tail -n 1) Review the settings in docker-compose.yml. Note that it is not default to store the postgresql database and redis databases in a persistent storage location. If you plan on running your instance in production, you must uncomment the volumes directive in docker-compose.yml. Getting the Mastodon image Using a prebuilt image If you're not making any local code changes or customizations on your instance, you can use a prebuilt Docker image to avoid the time and resource consumption of a build. Images are available from Docker Hub: https://hub.docker.com/r/tootsuite/mastodon/ To use the prebuilt images: Open docker-compose.yml in your favorite text editor. Comment out the build: . lines for all images (web, streaming, sidekiq). Edit the image: tootsuite/mastodon lines for all images to include the release you want. The default is latest which is the most recent stable version, however it recommended to explicitly pin a version: If you wanted to use v2.2.0 for example, you would edit the lines to say: image: tootsuite/mastodon:v2.2.0 Save the file and exit the text editor. Run cp .env.production.sample .env.production to bootstrap the configuration. You will need to edit this file later. Run docker-compose build. It will now pull the correct image from Docker Hub. Set correct file-owner with chown -R 991:991 public Building your own image You must build your own image if you've made any code modifications. To build your own image: Open docker-compose.yml in your favorite text editor. Uncomment the build: . lines for all images (web, streaming, sidekiq) if needed. Save the file and exit the text editor. Run cp .env.production.sample .env.production to bootstrap the configuration. You will need to edit this file later. Run docker-compose build. Set correct file-owner with chown -R 991:991 public Building the app Now the image can be used to generate a configuration with: docker-compose run --rm web bundle exec rake mastodon:setup This is an interactive wizard that will guide you through the basic and necessary options and generate new app secrets. At some point it will output your configuration, copy and paste that configuration into the .env.production file. The wizard will setup the database schema and precompile assets. After it's done, you can launch Mastodon with: docker-compose up -d Configuration Following that, make sure that you read the production guide, beginning with the section that describes how to point Nginx to Mastodon. The container has two volumes, for the assets and for user uploads, and optionally two more, for the postgresql and redis databases. The default docker-compose.yml maps them to the repository's public/assets and public/system directories, you may wish to put them somewhere else. Likewise, the PostgreSQL and Redis images have data containers that you may wish to map somewhere where you know how to find them and back them up. Note: The --rm option for docker-compose will remove the container that is created to run a one-off command after it completes. As data is stored in volumes it is not affected by that container clean-up. Running tasks Running any of these tasks via docker-compose would look like this: docker-compose run --rm web bundle exec rake mastodon:media:clear Updating This approach makes updating to the latest version a real breeze. git fetch to download updates from the repository. Now you need to tell git to use those updates. You have probably changed your docker-compose.yml file. Check with git status. If the docker-compose.yml file is modified, run git stash to stash your changes. git checkout TAG_NAME to use the tag code. (If you have committed changes, use git merge TAG_NAME instead, though this isn't likely.) Only if you ran git stash, now run git stash pop to redo your changes to docker-compose.yml. Double check the contents of this file. Build the updated Mastodon image. If you are using a prebuilt image: First, edit the image: tootsuite/mastodon lines in docker-compose.yml to include the tag for the new version. E.g. image: tootsuite/mastodon:v2.3.0 To pull the prebuilt image, or build your own from the updated code: docker-compose build (optional) docker-compose run --rm web bundle exec rake db:migrate to perform database migrations. Does nothing if your database is up to date. (optional) docker-compose run --rm web bundle exec rake assets:precompile to compile new JS and CSS assets. Follow any other special instructions in the release notes. docker-compose up -d to re-create (restart) containers and pick up the changes. ","link":"https://mashiro.top/post/mastodon-notes/"},{"title":"Git Notes","content":"# clean cache (.gitignore) git rm -r --cached . # add/set remote git remote add origin https://github.com/user/repo.git git remote set-url origin https://github.com/user/repo.git git remote -v ","link":"https://mashiro.top/post/git-notes/"},{"title":"Docker Frequently Used Commands","content":"docker pull gitlab/gitlab-ce:latest docker-compose up -d docker-compose ps ","link":"https://mashiro.top/post/docker-frequently-used-commands/"},{"title":"Private: Nginx Compile Configure","content":"make clean ./configure --prefix=/etc/nginx/ \\ --with-stream \\ --with-stream_realip_module \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-http_geoip_module \\ --with-http_auth_request_module \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --http-log-path=/var/log/nginx/access.log \\ --error-log-path=/var/log/nginx/error.log \\ --add-module=../modules/ngx_http_substitutions_filter_module \\ --add-dynamic-module=../modules/ngx_http_google_filter_module \\ --add-dynamic-module=../nginx/lua-nginx-module \\ --add-dynamic-module=../nginx/incubator-pagespeed-ngx-1.13.35.2-stable #dynamic make modules ls objs/ cp objs/ngx_pagespeed.so /usr/local/nginx/modules/ #cp objs/nginx /usr/local/nginx/sbin/ sed -i 's+ssl on;+#ssl on;+g' app/* ","link":"https://mashiro.top/post/private-nginx-compile-configure/"},{"title":"SSPanel Back-end (shadowsocks-mod) Installation","content":"Install GCC compile tool (build-essential) apt install build-essential Compile encryption library (libsodium) wget https://github.com/jedisct1/libsodium/releases/download/1.0.16/libsodium-1.0.16.tar.gz tar xf libsodium-1.0.16.tar.gz &amp;&amp; cd libsodium-1.0.16 ./configure &amp;&amp; make -j2 &amp;&amp; make install echo /usr/local/lib &gt; /etc/ld.so.conf.d/usr_local_lib.conf ldconfig Install shadowsocks-mod git clone https://github.com/mashirozx/shadowsocks-mod.git cd shadowsocks-mod git reset --hard efc986b731164aae3fef6fb5b9e8802754e63e3e pip install -r requirements.txt cp apiconfig.py userapiconfig.py cp config.json user-config.json If you see error info Command &quot;python setup.py egg_info&quot; failed with error code 1 when installing requirements, try: pip install --upgrade setuptools Configuration nano userapiconfig.py Database API NODE_ID = 0 API_INTERFACE = &quot;glzjinmod&quot; MYSQL_HOST = &quot;127.0.0.1&quot; MYSQL_PORT = 3306 MYSQL_USER = &quot;sspanel&quot; MYSQL_PASS = &quot;sspanel&quot; MYSQL_DB = &quot;sspanel&quot; Web API NODE_ID = 0 API_INTERFACE = &quot;modwebapi&quot; WEBAPI_URL = &quot;https://ssr.com&quot; WEBAPI_TOKEN = &quot;NimaQu&quot; Run ss server # test log server python server.py # run with log ./logrun.sh # run without log ./run.sh # view log ./tail.sh # stop ./stop.sh Cover: https://www.freepik.com/free-vector/vector-night-airport-with-plane-helicopter_3824040.htm ","link":"https://mashiro.top/post/sspanel-back-end-shadowsocks-mod-installation/"},{"title":"Nginx FAQ","content":"net::ERR_INCOMPLETE_CHUNKED_ENCODING 200 Fixed by adding proxy_max_temp_file_size 0; in server block. net::ERR_HTTP2_PROTOCOL_ERROR 200 The same issue as the issue ERR_INCOMPLETE_CHUNKED_ENCODING. The ERR_INCOMPLETE_CHUNKED_ENCODING happened on my server side, resource load directly from my server will be warmed as net::ERR_INCOMPLETE_CHUNKED_ENCODING 200 by chrome, and the same resource load from cloudflare will be warmed as net::ERR_HTTP2_PROTOCOL_ERROR 200. So the way to fix is the same, just adding proxy_max_temp_file_size 0; in server block. ","link":"https://mashiro.top/post/nginx-faq/"},{"title":"Android Studio + Kotlin: Unresolved references","content":"Android Studio 3.5 Kotlin Version 1.3.41 See the error message in both editor and the gradle compile error log. The solution is, delete ./.idea/libraries. Reference: https://stackoverflow.com/a/49743044/8083009 ","link":"https://mashiro.top/post/android-studio-kotlin-unresolved-references/"},{"title":"How to Fix \"The Frontend Does not Match Zabbix Database\"","content":"Err: The frontend does not match Zabbix database. Current database version (mandatory/optional): 3040000/4000003. Required mandatory version: 4000000. Contact your system administrator. When updating Zabbix from version 3.4 to version 4.0, I saw this on the Zabbix web page: Database error The frontend does not match Zabbix database. Current database version (mandatory/optional): 3040000/4000003. Required mandatory version: 4000000. Contact your system administrator. The solution is, login to database and run: mysql&gt; use zabbix; mysql&gt; update dbversion set mandatory=4000000; mysql&gt; flush privileges; See: https://blog.whsir.com/post-2357.html ","link":"https://mashiro.top/post/how-to-fix-the-frontend-does-not-match-zabbix-database/"},{"title":"A Complete JavaScript Cookie Version Control","content":"When adding some new features to my JavaScript, I find the remaining cookies are pretty hard to deal with that some of them were set by actions such as click, hover. So firstly I considered doing something to clean all cookie at once: if (getCookie(&quot;cookie_control&quot;)!=&quot;2018/5/11&quot;) { deleteAllCookies(); setCookie(&quot;cookie_control&quot;,&quot;2018/5/11&quot;,365); } But I found there is no such an easy deleteAllCookies() method! As this answer said: There is no 100% solution to delete browser cookies. The problem is that cookies are uniquely identified by not just by their key &quot;name&quot; but also their &quot;domain&quot; and &quot;path&quot;. Without knowing the &quot;domain&quot; and &quot;path&quot; of a cookie, you cannot reliably delete it. This information is not available through JavaScript's document.cookie. It's not available through the HTTP Cookie header either! So my idea is to add a Cookie version control with a full set of setting, getting and removing of cookies: var cookie_version_control = '---2018/5/11'; function setCookie(name,value,days) { var expires = &quot;&quot;; if (days) { var date = new Date(); date.setTime(date.getTime() + (days*24*60*60*1000)); expires = &quot;; expires=&quot; + date.toUTCString(); } document.cookie = name+cookie_version_control + &quot;=&quot; + (value || &quot;&quot;) + expires + &quot;; path=/&quot;; } function getCookie(name) { var nameEQ = name+cookie_version_control + &quot;=&quot;; var ca = document.cookie.split(';'); for(var i=0;i &lt; ca.length;i++) { var c = ca[i]; while (c.charAt(0)==' ') c = c.substring(1,c.length); if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length,c.length); } return null; } function removeCookie(name) { document.cookie = name+cookie_version_control+'=; Max-Age=-99999999;'; } Now everytime you want to update your cookies, it's OK just modifying the content of cookie_version_control. ","link":"https://mashiro.top/post/add-a-version-control-for-cookies/"},{"title":"The Machine Learning Landscape","content":"This &quot;Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow&quot; is the very first book I've read about ML. I know reading such a book at this time point is a bit out of my business, but maybe it won't be a waste of time if just do this in free time and keep making notes (in English, this is very important). The first chapter mainly contains basic knowledge of ML. Here the classification of ML model can be important because it decides how to build models for a specified problem. What Does ML Do? Well, I do think always determining a definition with broad and narrow definitions is pretty complex. And I think to answer this question these two graphs could be enough: So ML is a script that has the ability to self-strengthen. NOTE To summarize, Machine Learning is great for: Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better. Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution. Fluctuating environments: a Machine Learning system can adapt to new data. Getting insights about complex problems and large amounts of data. Types of ML Systems NOTE There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on: Whether or not they are trained with human supervision (supervised, unsupervised, semisupervised, and Reinforcement Learning) Whether or not they can learn incrementally on the fly (online versus batch learning) Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do (instance-based versus model-based learning) Supervised/Unsupervised Learning Supervised learning Normally, you train the system with a labeled dataset. The first kind of supervised learning task is classification, like a spam filter: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. And the second kind of supervised learning task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is called regression. To train the system, you need to give it many examples of cars, including both their predictors and their labels (i.e., their prices). So you see, this kind of ML is very important for us economists. NOTE In Machine Learning an attribute is a data type (e.g., ‚ÄúMileage‚Äù), while a feature has several meanings depending on the context, but generally means an attribute plus its value (e.g., ‚ÄúMileage = 15,000‚Äù). Many people use the words attribute and feature interchangeably, though. Here are some of the most important supervised learning algorithms: k-Nearest Neighbors Linear Regression Logistic Regression Support Vector Machines (SVMs) Decision Trees and Random Forests Neural networks2 Unsupervised learning* In unsupervised learning, the training data is unlabeled. Currently not understand this segment well, further study is needed. NOTE Here are some of the most important unsupervised learning algorithms: Clustering (ËÅöÁ±ª) k-Means Hierarchical Cluster Analysis (HCA) Expectation Maximization Visualization and dimensionality reduction Principal Component Analysis (PCA) Kernel PCA Locally-Linear Embedding (LLE) t-distributed Stochastic Neighbor Embedding (t-SNE) Association rule learning Apriori Eclat Semisupervised learning Anomaly detection (eg. detecting unusual payment) Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unlabeled data and a little bit of labeled data. A typical example is Google Photos. Reinforcement Learning* Learing by rewards and penalties. Further study needed. AlphaGo is a good example. Batch and Online Learning Whether or not the system can learn incrementally from a stream of incoming data. Batch learning Training off line and launching into production without learning any more. Every time to add new data to example dataset, the system need to scratch on the full dataset again, not just the new data, but also the old data. It's time token and resources occupation, so we need online learning. Online learning Training the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. It can also be used to train systems on huge datasets that cannot fit in one machine‚Äôs main memory. NOTE This whole process is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning. In online learning, you should be very cautious about bad data. Instance-Based Versus Model-Based Learning Categorizing Machine Learning systems is by how they generalize (‰∏ÄËà¨ÂåñÔºü). Most Machine Learning tasks are about making predictions. This means that given a number of training examples, the system needs to be able to generalize to examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal is to perform well on new instances. In one words, I think it means your algorithm could generally used to another datasets, not only to specified dataset. Actually this is why we do it with codes instead of Excel aha? Instance-based learning* Not understand well, only beacuse of my poor English? üò¶ In my currently understanding it means learning not only by the same label, but also the similar label. Model-based learning This is pretty like what we do on Excel‚Äîtry to regress with different functions. In this case, the functions are models, such like linear models and so on. This is the first example code making linear regression with Scikit-Learn: import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd import sklearn # Load the data oecd_bli = pd.read_csv(&quot;oecd_bli_2015.csv&quot;, thousands=',') gdp_per_capita = pd.read_csv(&quot;gdp_per_capita.csv&quot;,thousands=',',delimiter='\\t', encoding='latin1', na_values=&quot;n/a&quot;) # Prepare the data country_stats = prepare_country_stats(oecd_bli, gdp_per_capita) X = np.c_[country_stats[&quot;GDP per capita&quot;]] y = np.c_[country_stats[&quot;Life satisfaction&quot;]] # Visualize the data country_stats.plot(kind='scatter', x=&quot;GDP per capita&quot;, y='Life satisfaction') plt.show() # Select a linear model lin_reg_model = sklearn.linear_model.LinearRegression() # Train the model lin_reg_model.fit(X, y) # Make a prediction for Cyprus X_new = [[22587]] # Cyprus' GDP per capita print(lin_reg_model.predict(X_new)) # outputs [[ 5.96242338]] Main Challenges of Machine Learning Bad algorithm vs. bad data. Insufficient Quantity of Training Data Quantity is important. There's also an interesting research find that a huge number of data could make up the shortages on algorithm. If you have a lot of data money, you can do whatever you like. XD Nonrepresentative Training Data Statistically, they are extreme value. Poor-Quality Data Irrelevant Features Means too many features with too little data. Overfitting the Training Data Underfitting the Training Data -EOF- I find it's a bit slow trying to write every part in detail, maybe I was really too meticulous as writing a reading note, next time I'll try to conclude in my own words instead of using lots of words directly from the book. ","link":"https://mashiro.top/post/the-machine-learning-landscape/"},{"title":"Hello Gridea","content":"üëè Welcome to use Gridea ÔºÅ ‚úçÔ∏è Gridea A static blog writing client. You can use it to record your life, mood, knowledge, notes and ideas... Featuresüëá üìù Use the coolest Markdown grammar to create quickly üåâ Insert pictures and article cover charts anywhere in the article üè∑Ô∏è Label and group articles üìã Customize menus and even create external link menus üíª Use this client on Windows or MacOS or Linux üåé Use Github Pages or Coding Pages to show the world that more platforms will be supported in the future üí¨ Simply configure and access the Gitalk or DisqusJS comment system üá¨üáß Use simplified Chinese„ÄÅtraditional Chinese„ÄÅ English üåÅ Use any default theme within the application or any third-party theme, free theme customization üñ• Customize the source folder and synchronize multiple devices using OneDrive, iCloud, Dropbox, etc. üå± Of course Gridea is still very young and has many shortcomings, but please believe it will keep moving forward üèÉ In the future, it will surely become your inseparable partner Give full play to your talentsÔºÅ üòò Enjoy~ ","link":"https://mashiro.top/post/hello-gridea/"}]}